\documentclass{article}

\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{tabularx}
\usepackage{graphicx}

\fancypagestyle{firstpage}{%
  \lhead{ETH Zürich}
  \rhead{Frühjahrssemester 2023}
}

\title{Acquiring Linguistic Knowledge from Multimodal Input\\} 

\author{\href{mailto:tamariucai@student.ethz.ch}{Theodor Amariucai} (21-943-337) - \emph{MSc Computer Science}}

\date{\vspace{-5ex}} %NO DATE

\begin{document}
\maketitle
\thispagestyle{firstpage}

\section*{Supervision}

\begin{itemize}
\renewcommand\labelitemi{--}
    \item Alexander Warstadt (\href{mailto:alexanderscott.warstadt@inf.ethz.ch}{alexanderscott.warstadt@inf.ethz.ch})
    \item Ryan Cotterell (\href{mailto:ryan.cotterell@inf.ethz.ch}{ryan.cotterell@inf.ethz.ch})
\end{itemize}

\section*{Introduction}

Natural language processing (NLP) is a field of machine learning that has seen tremendous progress in recent years. Pretraining has become a popular approach in NLP, where models are trained on large amounts of unsupervised data to learn general linguistic knowledge and improve their performance on downstream tasks. However, the current trend of pretraining language models (LMs) largely relies on text \cite{BERT, DeBERTa, RoBERTa} (and to a lesser extent, vision \cite{FLAVA, Bugliarello2020, ViLBERT}) input, overlooking other influential sensory data (e.g., speech) that is part of the learning signal for humans acquiring language.

Multimodal language models have been gaining attention due to their ability to reason about information transcending text. By incorporating multiple modalities, they can better capture the complex relationships between words and the visual or auditory world, obtaining a more comprehensive and accurate understanding of concepts.

\section*{Motivation}

Existing Transformer-based \cite{Vaswani2017}, pre-trained vision and language models have achieved impressive performance at vision tasks and cross- and multimodal vision and language tasks \cite{FLAVA}. However, even the most recent such models appear unlikely to improve natural language understanding (NLU) over their unimodal (text-only) counterparts \cite{BrownStudy, AlexThesis}. Moreover, large models are, in general, significantly less data-efficient than humans at tasks such as grammar learning, requiring hundreds of times more exposure to words than a child \cite{AlexThesis}.

Human language acquisition goes beyond simple image-text pairs and is driven by additional stimuli (e.g., hearing). However, speech-text multimodal architectures \cite{SLAM} are an under-explored area in language acquisition research, with the incentives typically being automated speech recognition or translation \cite{SpeechT5, SpeechUT, MetaASRPaper}. This could be a missed opportunity, as information embedded in speech wavelengths might more effectively reinforce grammaticality in models by more closely mapping to text (as opposed to visual environments).

We hypothesize that learning from multimodal inputs could improve language acquisition and narrow the gap between how humans and machines learn. This hypothesis has only been tested in limited pretraining paradigms \cite{BrownStudy} or incidental ablation studies with restricted evaluation and analysis \cite{FLAVA}. 

Lastly, we seek to understand the impact of varying quantities of text and multimodal training data on the linguistic competence of language models, and to find whether efficient trade-offs could therefore be made in the input space. % The motivation behind training multimodal models has rarely been to improve language learning

\section*{Target of the Thesis}

The goal is to investigate a state-of-the-art multimodal language model and enrich it with audio capabilities. The final deliverable is a controlled study backed by a novel, open source, speech-text model whose NLU \cite{BLiMP, CoLA} and pseudo-log-likelihoods \cite{Salazar_2020} are scrutinized under different sensory groundings. 

By accessing new insights into the fundamental challenges of language modeling, we hope to foster innovation among NLP researchers, AI practitioners, computational linguists, and cognitive scientists. 

% Acknowledge the following limitation: the way we will introduce images and audio isn't representative of all the learning signal in that modality, but rather is limited by our choice of data and pretraining objectives. So whichever we find is more helpful, it might just be because we didn't have the right data or objective.

\section*{Objectives}

\begin{enumerate}
    \item \textbf{Literature review} on the state-of-the-art in multimodal language model pretraining, including common objectives, benchmarks, and corpora. 
    \item Code \textbf{infrastructure setup} and controlled \textbf{experimentation} (minimizing differences in model sizes and training data):
    \begin{enumerate}
        \item Can trade-offs be made in the vision-text input space while preserving performance? Ablation study \footnote{An option would be to 1) set text-only baselines, 2) independently vary text and non-text training data, 3) evaluate on text data.} on how FLAVA's \footnote{FLAVA is a recent breakthrough \cite{BrownStudy, AlexThesis} vision-text model that was shown to improve NLU.} linguistic knowledge is affected by varying volumes of text and visual input. % (link to BabyLM here?).
    \end{enumerate}
    \item \textbf{Development} 
   \begin{enumerate}
        \item Consider SLAM \cite{SLAM}, a closed source, early-fusion, speech-text model where NLU performance was shown to decrease after adding speech input. We respond to the authors' call for "better cross-modal alignments to alleviate the presumed capacity limitations" by enriching FLAVA with audio processing capabilities. Thus, we develop the first \footnote{As far as my current literature review suggests.} open source, early-fusion, speech-text model. %(e.g., following data2vec \cite{data2vec_20}) (with a different MMM task)
        \item Can trade-offs be made in the \textit{speech}-text input space while preserving performance? Ablation study on how the new model's linguistic knowledge is affected by varying volumes of text and speech input.
    \end{enumerate}
    \item \textbf{Analysis and thesis writing} 
       \begin{enumerate}
         \item Points of interest will be grammaticality (CoLA \cite{CoLA}, BLiMP \cite{BLiMP}) and data efficiency (\# words the model has been exposed to).
       \end{enumerate}
\end{enumerate}

\section*{Deliverables}

The project will result in the following concrete deliverables:
\begin{itemize}
    \item Project description 
   \begin{itemize}
       \item Introduction / motivation of the problem
       \item Thorough analysis of related work
       \item Description of the resulting implementation including its design
       \item Experimental evaluation
   \end{itemize}
    \item Complete source code for the resulting prototype, benchmarking, data analysis, and scripts \footnote{This is accompanied with enough documentation to allow complete reproduction of the experimental results.}. % Release of the model(s) to HuggingFace.
    \item Presentation of the results and demonstration of functionality.
\end{itemize}

% Additionally, the findings will inform the design of future multimodal models, allowing practitioners and researchers to make informed decisions about the modalities to include and the volume of data required to achieve optimal performance.

\section*{Grading}

The Master's Thesis (MT) is a graded semester performance. In order to successfully complete the MT, a grade of 4.0 or higher must be obtained.  The supervisor establishes the assessment criteria in a written report, which can include a presentation. In principle, the following evaluation scale is applied: 
\begin{figure}[h!]
    \begin{tabular}{ |l p{10cm}| }
        \hline  Grade & Requirements  \\ \hline \hline
        6.00  & Work and results are publishable for international workshops  \\ \hline
        5.50 & Thesis quality significantly exceeds expectations  \\ \hline
        5.00 & Thesis meets expectations  \\ \hline
        4.50 & Thesis partially meets expectations and has minor deficits   \\ \hline
        4.00 & Thesis meets minimum quality requirements; but has major deficits  and  is  clearly  below expectations \\ \hline 
    \end{tabular}
\end{figure}

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
