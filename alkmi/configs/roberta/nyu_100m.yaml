_target_: alkmi.definitions.AblationArguments

text_perc: 100
vision_perc: 0

training:
  _target_: alkmi.definitions.TrainingArguments
  lightning:
    devices: -1
    val_check_interval: 8000
    limit_val_batches: 2000 # REQUIRED, otherwise the validation loop doesn't terminate
    accumulate_grad_batches: 5
    strategy: ddp
    enable_progress_bar: false
  lightning_checkpoint:
    dirpath: "/cluster/work/cotterell/tamariucai/HuggingfaceCheckpoints/roberta-nyu_100m/"
    filename: roberta-{epoch:02d}-{step}
  lightning_load_from_checkpoint: null
  seed: -1
  batch_size: 52
  num_workers: 2
  use_wandb: true
  # https://huggingface.co/nyu-mll/roberta-base-100M-2, Table 9
  learning_rate: 1e-4 # or 5e-4 for batch size 2048 (https://github.com/facebookresearch/fairseq/blob/main/examples/roberta/README.pretraining.md)
  adam_eps: 1e-6
  adam_weight_decay: 1e-2
  adam_betas: [ 0.9, 0.98 ]
  warmup_steps: 2000

datasets:
  _target_: alkmi.definitions.TrainingDatasetsInfo
  ablation:
    # For VLDataModule, column "image" either needs to have an image (not the case for "facebook/pmd", "wit"), or be missing
    _target_: alkmi.definitions.TrainingSingleDatasetInfo
    train:
      - _target_: alkmi.definitions.HFDatasetInfo
        key: theodor1289/nyu_100m
        split_key_mapping: # neither 'validation' nor 'test' splits exist for nyu_100m
          validation: train
    val:
      - _target_: alkmi.definitions.HFDatasetInfo
        key: openwebtext
        split_key_mapping: # neither 'validation' nor 'test' splits exist for openwebtext
          validation: train
    datamodule_extra_kwargs: # This key is used in the MLMDataModule
      text_columns: [ "text" ]

model:
  _target_: alkmi.definitions.ModelArguments
  pretrained: null
  mlm_perc: 0.4  # Sweep: https://wandb.ai/rycolab/multimodal-wit/sweeps/k84pvx6y/workspace?workspace=user-tamariucai
  name: roberta