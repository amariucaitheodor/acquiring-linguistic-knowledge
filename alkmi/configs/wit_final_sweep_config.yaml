program: train.py
name: flava-textvision-wit
description: FLAVA (text-vision) sweep run on the WiT dataset for the final ablation study.
project: multimodal-wit
run_cap: 200
entity: rycolab
method: grid
metric:
  name: evaluation/pseudo_perplexity
  goal: minimize
command:
  - ${env}
  - ${interpreter} # makes sure the right interpreter is used on Euler (3.10 req. for BLiMP)
  - -m
  - train
  - config=configs/wit.yaml
# We don't want to early_terminate here (across configs) because some configs will be naturally worse and that's okay.
# What we do want is over-fitting detection, but that's done on the average validation loss using callbacks.
parameters:
  text_perc:
    # 59% is approx. 100M words, 0% would be relevant just for completeness
    values: [ 1, 10, 59, 100 ]
  vision_perc:
    values: [ 0, 1, 10, 100 ]
  seed: # Repeat each run three times with different seeds (then take average)
    distribution: categorical
    values: [ 5501650, 1930309, 7865220 ]